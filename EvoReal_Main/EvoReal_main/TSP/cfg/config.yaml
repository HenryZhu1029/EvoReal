defaults:
  - _self_
  - llm_client: openai

hydra:
  run:
    dir: ./hydra_outputs/${problem.task_name}-${now:%Y-%m-%d_%H-%M-%S}
  job:
    chdir: False
  output_subdir: null
# Define Problem
problem: 
  problem_name: tsp
  obj_type: min
  task_name: tspdata_generator_evo

# The chosen algorithm
model: lehd  # or pomo

max_iter: 10 
# Main tspdata_evo loop parameters
pop_size: 10 # population size 
init_pop_size: 25 # initial code population size  (not recommend for a number above 40 or below 5)
max_code_runs: 100
mutation_rate: 0.5 # mutation rate for code population
num_batches: 4 # number of batches to initialize population

temperature: 1
cuda_device_num: 0  #can be an interger or a list of integers
toleration: 3

module_to_evo: "S1"

# multi_processing parameters for concorde for lehd
num_training_problems: 10000 # problems generated by each module for training
n_proc: 5 # number of multi-processing tasks; recommend numbers of processing: [1,10]
# Prompts
prompts: 

  population_generator_prompt: |
    ### Task Objective:
    {problem_desc}

    ### Function Name:
    {func_name}
    
    ### Function Description:
    {func_desc}
  
    ### Function Signature:
    {func_signature}
  
    ### Design Guidance:
    - Design generation strategies that reflect spatial complexity, distributional realism, and diverse patterns across samples.
    - Encourage variability across samples: avoid excessive reuse of the same generation logic or structure.

    ### Reference Module:
    {seed} 
    Before returning `all_points`, you must guarantee that each sample contains exactly `problem_size` 2D coordinates by either truncating (via `torch.randperm`) or padding (via `torch.cat` + `torch.rand`).
    Ensure all tensors have shape `(problem_size, 2)` and are stackable across batch dimension. Refer to the seed function above for how to pad or truncate.